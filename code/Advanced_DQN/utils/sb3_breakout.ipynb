{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 26.3 GiB for an array with shape (250000, 4, 4, 84, 84) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Robin\\Documents\\1_EPFL\\MA3\\FF_DRL\\code\\Advanced_DRL\\sb3_breakout.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Robin/Documents/1_EPFL/MA3/FF_DRL/code/Advanced_DRL/sb3_breakout.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Frame-stacking with 4 frames\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Robin/Documents/1_EPFL/MA3/FF_DRL/code/Advanced_DRL/sb3_breakout.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m vec_env \u001b[39m=\u001b[39m VecFrameStack(vec_env, n_stack\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Robin/Documents/1_EPFL/MA3/FF_DRL/code/Advanced_DRL/sb3_breakout.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model \u001b[39m=\u001b[39m DQN(\u001b[39m\"\u001b[39;49m\u001b[39mCnnPolicy\u001b[39;49m\u001b[39m\"\u001b[39;49m, vec_env, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Robin/Documents/1_EPFL/MA3/FF_DRL/code/Advanced_DRL/sb3_breakout.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39m25_000\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Robin/Documents/1_EPFL/MA3/FF_DRL/code/Advanced_DRL/sb3_breakout.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m obs \u001b[39m=\u001b[39m vec_env\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[1;32mc:\\Users\\Robin\\anaconda3\\envs\\FF_DRL\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:141\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, target_update_interval, exploration_fraction, exploration_initial_eps, exploration_final_eps, max_grad_norm, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration_rate \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    140\u001b[0m \u001b[39mif\u001b[39;00m _init_setup_model:\n\u001b[1;32m--> 141\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_model()\n",
      "File \u001b[1;32mc:\\Users\\Robin\\anaconda3\\envs\\FF_DRL\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:144\u001b[0m, in \u001b[0;36mDQN._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_setup_model\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_setup_model()\n\u001b[0;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_aliases()\n\u001b[0;32m    146\u001b[0m     \u001b[39m# Copy running stats, see GH issue #996\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Robin\\anaconda3\\envs\\FF_DRL\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:189\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mYou must pass an environment when using `HerReplayBuffer`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m         replay_buffer_kwargs[\u001b[39m\"\u001b[39m\u001b[39menv\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\n\u001b[1;32m--> 189\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer_class(\n\u001b[0;32m    190\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size,\n\u001b[0;32m    191\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space,\n\u001b[0;32m    192\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space,\n\u001b[0;32m    193\u001b[0m         device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice,\n\u001b[0;32m    194\u001b[0m         n_envs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs,\n\u001b[0;32m    195\u001b[0m         optimize_memory_usage\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimize_memory_usage,\n\u001b[0;32m    196\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mreplay_buffer_kwargs,\n\u001b[0;32m    197\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_class(\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space,\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space,\n\u001b[0;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_schedule,\n\u001b[0;32m    203\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_kwargs,\n\u001b[0;32m    204\u001b[0m )\n\u001b[0;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\Robin\\anaconda3\\envs\\FF_DRL\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:216\u001b[0m, in \u001b[0;36mReplayBuffer.__init__\u001b[1;34m(self, buffer_size, observation_space, action_space, device, n_envs, optimize_memory_usage, handle_timeout_termination)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_shape), dtype\u001b[39m=\u001b[39mobservation_space\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    214\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m optimize_memory_usage:\n\u001b[0;32m    215\u001b[0m     \u001b[39m# When optimizing memory, `observations` contains also the next observation\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_observations \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mzeros((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_envs, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobs_shape), dtype\u001b[39m=\u001b[39;49mobservation_space\u001b[39m.\u001b[39;49mdtype)\n\u001b[0;32m    218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\n\u001b[0;32m    219\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dim), dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_dtype(action_space\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    220\u001b[0m )\n\u001b[0;32m    222\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 26.3 GiB for an array with shape (250000, 4, 4, 84, 84) and data type uint8"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "# There already exists an environment generator\n",
    "# that will make and wrap atari environments correctly.\n",
    "# Here we are also multi-worker training (n_envs=4 => 4 environments)\n",
    "vec_env = make_atari_env(\"BreakoutNoFrameskip-v4\", n_envs=4, seed=0)\n",
    "# Frame-stacking with 4 frames\n",
    "vec_env = VecFrameStack(vec_env, n_stack=4)\n",
    "\n",
    "model = DQN(\"CnnPolicy\", vec_env, verbose=1)\n",
    "model.learn(total_timesteps=25_000)\n",
    "\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FF_DRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
